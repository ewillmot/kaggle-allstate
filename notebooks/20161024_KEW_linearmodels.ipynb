{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc = '/Users/elena/Documents/Kaggle/Allstate/data/'\n",
    "train = pd.read_csv(os.path.join(loc,'train.csv'))\n",
    "train.drop('id',axis=1,inplace=True)\n",
    "\n",
    "test = pd.read_csv(os.path.join(loc,'test.csv'))\n",
    "ids = test['id']\n",
    "test.drop('id',axis=1,inplace=True)\n",
    "\n",
    "submission = pd.read_csv(os.path.join(loc,\"sample_submission.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical:  116\n",
      "Continuous:  14\n"
     ]
    }
   ],
   "source": [
    "# List of categorical columns\n",
    "keepcats = [col for col in train.columns if 'cat' in col]\n",
    "# List of continuous columns\n",
    "keepcont = [col for col in train.columns if 'cont' in col]\n",
    "\n",
    "print 'Categorical: ',len(keepcats)\n",
    "print 'Continuous: ',len(keepcont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elena/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cont1</th>\n",
       "      <th>cont2</th>\n",
       "      <th>cont3</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>log-loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.726300</td>\n",
       "      <td>0.245921</td>\n",
       "      <td>0.187583</td>\n",
       "      <td>0.789639</td>\n",
       "      <td>0.310061</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>7.702186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.330514</td>\n",
       "      <td>0.737068</td>\n",
       "      <td>0.592681</td>\n",
       "      <td>0.614134</td>\n",
       "      <td>0.885834</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>7.157424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.261841</td>\n",
       "      <td>0.358319</td>\n",
       "      <td>0.484196</td>\n",
       "      <td>0.236924</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>8.008063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.321594</td>\n",
       "      <td>0.555782</td>\n",
       "      <td>0.527991</td>\n",
       "      <td>0.373816</td>\n",
       "      <td>0.422268</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>6.845720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.273204</td>\n",
       "      <td>0.159990</td>\n",
       "      <td>0.527991</td>\n",
       "      <td>0.473202</td>\n",
       "      <td>0.704268</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>7.924380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cont1     cont2     cont3     cont4     cont5     cont6     cont7  \\\n",
       "0  0.726300  0.245921  0.187583  0.789639  0.310061  0.718367  0.335060   \n",
       "1  0.330514  0.737068  0.592681  0.614134  0.885834  0.438917  0.436585   \n",
       "2  0.261841  0.358319  0.484196  0.236924  0.397069  0.289648  0.315545   \n",
       "3  0.321594  0.555782  0.527991  0.373816  0.422268  0.440945  0.391128   \n",
       "4  0.273204  0.159990  0.527991  0.473202  0.704268  0.178193  0.247408   \n",
       "\n",
       "     cont8    cont9   cont10    cont11    cont12    cont13    cont14  log-loss  \n",
       "0  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493  0.714843  7.702186  \n",
       "1  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431  0.304496  7.157424  \n",
       "2  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709  0.774425  8.008063  \n",
       "3  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077  0.602642  6.845720  \n",
       "4  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011  0.432606  7.924380  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keeper = deepcopy(keepcont)\n",
    "keeper.append('loss')\n",
    "dataset = train[keeper]\n",
    "dataset['loss'] = np.log(dataset['loss'])\n",
    "dataset.rename(columns={'loss':'log-loss'},inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176\n",
      "(188318, 1176)\n"
     ]
    }
   ],
   "source": [
    "## Now let's code up the categorical data (0's and 1's instead of ABC's)\n",
    "labels = []\n",
    "cols = train.columns\n",
    "newlabels = []\n",
    "for i in range(0,116):\n",
    "    trainer = train[cols[i]].unique()\n",
    "    tester = test[cols[i]].unique()\n",
    "    # We want the intersection of labels from the training and testing datasets\n",
    "    # in case there are labels in the test dataset that don't show up in the training dataset.\n",
    "    labels.append(list(set(trainer) | set(tester)))    \n",
    "    \n",
    "    for thing in labels[-1]:\n",
    "        newlabels.append(cols[i]+'-'+str(thing))\n",
    "\n",
    "#One hot encode all categorical attributes\n",
    "cats = []\n",
    "for i in range(0, 116):\n",
    "    #Label encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(labels[i])\n",
    "    feature = label_encoder.transform(train.iloc[:,i])\n",
    "    feature = feature.reshape(train.shape[0], 1)\n",
    "    \n",
    "    \n",
    "    #One hot encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\n",
    "    feature = onehot_encoder.fit_transform(feature)\n",
    "    cats.append(feature)\n",
    "\n",
    "# Make a 2D array from a list of 1D arrays\n",
    "encoded_cats = np.column_stack(cats)\n",
    "print len(newlabels)\n",
    "print encoded_cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cont1</th>\n",
       "      <th>cont2</th>\n",
       "      <th>cont3</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>...</th>\n",
       "      <th>cat116-AP</th>\n",
       "      <th>cat116-AS</th>\n",
       "      <th>cat116-AR</th>\n",
       "      <th>cat116-AU</th>\n",
       "      <th>cat116-AT</th>\n",
       "      <th>cat116-AW</th>\n",
       "      <th>cat116-AV</th>\n",
       "      <th>cat116-AY</th>\n",
       "      <th>cat116-AX</th>\n",
       "      <th>cat116-N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.726300</td>\n",
       "      <td>0.245921</td>\n",
       "      <td>0.187583</td>\n",
       "      <td>0.789639</td>\n",
       "      <td>0.310061</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.330514</td>\n",
       "      <td>0.737068</td>\n",
       "      <td>0.592681</td>\n",
       "      <td>0.614134</td>\n",
       "      <td>0.885834</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.261841</td>\n",
       "      <td>0.358319</td>\n",
       "      <td>0.484196</td>\n",
       "      <td>0.236924</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.321594</td>\n",
       "      <td>0.555782</td>\n",
       "      <td>0.527991</td>\n",
       "      <td>0.373816</td>\n",
       "      <td>0.422268</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.273204</td>\n",
       "      <td>0.159990</td>\n",
       "      <td>0.527991</td>\n",
       "      <td>0.473202</td>\n",
       "      <td>0.704268</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cont1     cont2     cont3     cont4     cont5     cont6     cont7  \\\n",
       "0  0.726300  0.245921  0.187583  0.789639  0.310061  0.718367  0.335060   \n",
       "1  0.330514  0.737068  0.592681  0.614134  0.885834  0.438917  0.436585   \n",
       "2  0.261841  0.358319  0.484196  0.236924  0.397069  0.289648  0.315545   \n",
       "3  0.321594  0.555782  0.527991  0.373816  0.422268  0.440945  0.391128   \n",
       "4  0.273204  0.159990  0.527991  0.473202  0.704268  0.178193  0.247408   \n",
       "\n",
       "     cont8    cont9   cont10    ...     cat116-AP  cat116-AS  cat116-AR  \\\n",
       "0  0.30260  0.67135  0.83510    ...           0.0        0.0        0.0   \n",
       "1  0.60087  0.35127  0.43919    ...           0.0        0.0        0.0   \n",
       "2  0.27320  0.26076  0.32446    ...           0.0        0.0        0.0   \n",
       "3  0.31796  0.32128  0.44467    ...           0.0        0.0        0.0   \n",
       "4  0.24564  0.22089  0.21230    ...           0.0        0.0        0.0   \n",
       "\n",
       "   cat116-AU  cat116-AT  cat116-AW  cat116-AV  cat116-AY  cat116-AX  cat116-N  \n",
       "0        0.0        0.0        0.0        0.0        0.0        0.0       0.0  \n",
       "1        0.0        0.0        0.0        0.0        0.0        0.0       0.0  \n",
       "2        0.0        0.0        0.0        0.0        0.0        0.0       0.0  \n",
       "3        0.0        0.0        0.0        0.0        0.0        0.0       0.0  \n",
       "4        0.0        0.0        0.0        0.0        0.0        0.0       0.0  \n",
       "\n",
       "[5 rows x 1191 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(encoded_cats,columns=newlabels)\n",
    "final_data = pd.concat([dataset,df],axis=1)\n",
    "final_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = final_data['log-loss']\n",
    "x = final_data.drop('log-loss',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-45049dd41529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/elena/anaconda/lib/python2.7/site-packages/sklearn/metrics/regression.pyc\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \"\"\"\n\u001b[1;32m    162\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 163\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    164\u001b[0m     output_errors = np.average(np.abs(y_pred - y_true),\n\u001b[1;32m    165\u001b[0m                                weights=sample_weight, axis=0)\n",
      "\u001b[0;32m/Users/elena/anaconda/lib/python2.7/site-packages/sklearn/metrics/regression.pyc\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/elena/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    396\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/elena/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     52\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     53\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 54\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.5)\n",
    "\n",
    "newmodel = LinearRegression(normalize=True)\n",
    "newmodel.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print mean_absolute_error(np.exp(y_test),np.exp(newmodel.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x124fd2c50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqCtlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118b3c510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_test,newmodel.predict(X_test))\n",
    "plt.ylabel('Model predicted')\n",
    "plt.xlabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
